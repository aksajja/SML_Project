# -*- coding: utf-8 -*-
"""GuyBresler.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1YxnD7ybBTz5TpSuvMgzGU7Mc7J90q3MA
"""

import numpy as np
import pandas as pd
import math
from Experiments.sampling_from_ising import ising_samples
from utils import plot_error_vs_samples

def compute_num_samples_needed(p, zeta, max_deg, max_h, alpha, beta):
  '''
    p - number of nodes
    Ising params - alpha, beta, max_h
    error - zeta
  '''
  delta = 0.5*math.e**(-2*(beta*max_deg+max_h))
  tau = (alpha**2)*(delta**(4*max_deg+1))/(16*max_deg*beta)
  epsilon = tau/2
  l = 8/(tau**2)

  n = ((144*(l+3))/(epsilon**2 * delta**(2*l))) * math.log(p/zeta)
  return n

def compute_frequentist_probability(nodes_config: dict, samples: np.ndarray):
  '''
  Arguments -
    nodes_config - Dictionary of nodes (node_u,node_i,subset) 
                  and their configurations. 
    samples - Training set.
  
  Returns -
    Frequentist probability for a given configuration.
  '''
  num_samples = samples.shape[0]
  numerator = len(samples[np.where(np.all(\
      samples[:,list(nodes_config.keys())]==list(nodes_config.values()),axis=1))])
  
  return numerator/num_samples

def compute_emp_nu(node_u, node_i, x_s: dict, sample_set):
  # We assume +1 configuration to be 1 and -1 to be 0.
  num_samples = sample_set.shape[0]
  # merge dicts by, d = {**dict1,**dict2}
  
  # P(u_pos, i_pos,samples)
  p_all_joint_i_pos = compute_frequentist_probability({**{node_u:1,node_i:1},**x_s},sample_set)
  # P(i_pos,samples)
  p_joint_i_pos_samples = compute_frequentist_probability({**{node_i:1},**x_s},sample_set)
  
  # P(u_pos, i_neg,samples)
  p_all_joint_i_neg = compute_frequentist_probability({**{node_u:1,node_i:-1},**x_s},sample_set)
  # P(i_neg,samples)
  p_joint_i_neg_samples = compute_frequentist_probability({**{node_i:-1},**x_s},sample_set)
  
  # P(u_pos|(i_pos,samples)) =  P(u_pos, i_pos,samples)/P(i_pos,samples)
  emp_prob_one =  p_all_joint_i_pos/p_joint_i_pos_samples 
  # P(u_pos|(i_neg,samples)) =  P(u_pos, i_neg,samples)/P(i_neg,samples)
  emp_prob_two =  p_all_joint_i_neg/p_joint_i_neg_samples 
  
  return emp_prob_one-emp_prob_two

def compute_lambda_i(node_i, x_s: dict,sample_set):
  emp_prob_one = compute_frequentist_probability({**{node_i:1},**x_s},sample_set)
  emp_prob_two = compute_frequentist_probability({**{node_i:0},**x_s},sample_set)

  return 2*emp_prob_one*emp_prob_two

def compute_avg_emp_nu(node_u, node_i, sample_set, subset):
  # Average conditional influence - 
  # Weighted avg of absolute value of conditional influences.
  emp_nu_list = []
  # Get all possible values the random variable X_s can take on.
  lambda_i = []
  if len(subset)<1:
    emp_nu_list.append(compute_emp_nu(node_u,node_i,{},sample_set))
    lambda_i.append(compute_lambda_i(node_i,{},sample_set))
  else:
    X_s_labels = np.unique(sample_set[:,subset], axis=0)
    for x_s in X_s_labels:
      x_s_config = {k:v for k,v in zip(subset,x_s)}
      emp_nu_list.append(compute_emp_nu(node_u,node_i,x_s_config,sample_set))
      lambda_i.append(compute_lambda_i(node_i,x_s_config,sample_set))
  
  # avg of lambda_i * abs(nu)]
  return np.asarray(emp_nu_list).dot(np.asarray(lambda_i).T)

def pruning(subset, tau):
  for node_i in subset:
    if compute_avg_emp_nu(node_u,node_i,sample_set,subset)<tau:
      subset.remove(node_i)

  return subset

def learn_neighborhood(node_u,p,sample_set,tau):
  subset = []

  while True:
      influence = []
      for node_i in range(p):
        if node_i==node_u or node_i in subset:
          influence.append(-99999)
          continue
        influence.append(compute_avg_emp_nu(node_u,node_i,sample_set,subset))

      eta_opt = max(influence)
      i_opt = influence.index(eta_opt)

      if eta_opt<tau:
        return pruning(subset, tau)
      
      subset.append(i_opt)

def compute_error(label,pred):
  return np.count_nonzero(label!=pred)/label.shape[0]**2

def exp_guybresler(num_nodes, num_samples, zeta, alpha, beta):

  for p in num_nodes:
    errors = []
    for n in num_samples:
      max_deg = int(0.2*p)
      theta,sample_set = ising_samples(p, n, alpha, beta, max_deg)
      max_h = np.max(np.diag(theta))
      # compute_num_samples_needed(p, zeta, max_deg, max_h, alpha, beta)
      # Parameters
      delta = 0.5*math.e**(-2*(beta*max_deg+max_h))
      tau = (alpha**2)*(delta**(4*max_deg+1))/(16*max_deg*beta)

      neighborhoods = []
      for node_u in range(p):
        neighborhoods.append(learn_neighborhood(node_u,p,sample_set,tau))

      neighborhoods = np.asarray(neighborhoods)
      pred_prec_mat = (neighborhoods>0)*1
      err = compute_error(theta,pred_prec_mat)
      print(f'Sample complexity bound holds? Pred_err:{err}, Zeta:{zeta} ')
      errors.append(err)
    plot_error_vs_samples('GB',errors,num_samples,p)